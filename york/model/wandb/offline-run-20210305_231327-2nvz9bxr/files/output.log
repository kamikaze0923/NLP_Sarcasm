  0%|                                                                                                                                                                                                      | 0/10 [00:00<?, ?it/s]TypeError("forward() got an unexpected keyword argument 'attention_mask'")
> [0;32m/home/zijiao/anaconda3/envs/dnlp/lib/python3.8/site-packages/torch/nn/modules/module.py[0m(727)[0;36m_call_impl[0;34m()[0m
[0;32m    726 [0;31m        [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 727 [0;31m            [0mresult[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mforward[0m[0;34m([0m[0;34m*[0m[0minput[0m[0;34m,[0m [0;34m**[0m[0mkwargs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    728 [0;31m        for hook in itertools.chain(
[0m
> [0;32m/home/zijiao/anaconda3/envs/dnlp/lib/python3.8/site-packages/transformers/trainer.py[0m(1334)[0;36mcompute_loss[0;34m()[0m
[0;32m   1333 [0;31m            [0mlabels[0m [0;34m=[0m [0;32mNone[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m-> 1334 [0;31m        [0moutputs[0m [0;34m=[0m [0mmodel[0m[0;34m([0m[0;34m**[0m[0minputs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m   1335 [0;31m        [0;31m# Save past state if it exists[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
> [0;32m/home/zijiao/anaconda3/envs/dnlp/lib/python3.8/site-packages/transformers/trainer.py[0m(1304)[0;36mtraining_step[0;34m()[0m
[0;32m   1303 [0;31m        [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m-> 1304 [0;31m            [0mloss[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mcompute_loss[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0minputs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m   1305 [0;31m[0;34m[0m[0m
[0m
{'input_ids': tensor([[50259,    40,  1101,   407, 20536,   284,   423,   616,  1641,  3187,
            13, 50260, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,
         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,
         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,
         50256, 50256, 50256, 50256, 50256, 50256, 50257, 35474,   329,  1641,
         10013,  1303,    86,  2238,  1303,   400,    81,  2967,  2638,  1378,
            83,    13,  1073,    14,    52,  2953, 41473, 16072,    65,    50,
            23, 50258, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,
         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,
         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]],
       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1]], device='cuda:0'), 'labels': tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100, 50257, 35474,   329,  1641,
         10013,  1303,    86,  2238,  1303,   400,    81,  2967,  2638,  1378,
            83,    13,  1073,    14,    52,  2953, 41473, 16072,    65,    50,
            23, 50258,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]],
       device='cuda:0')}
[1;32m   1299 [0m[0;34m[0m[0m
[1;32m   1300 [0m        [0;32mif[0m [0mself[0m[0;34m.[0m[0muse_amp[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m   1301 [0m            [0;32mwith[0m [0mautocast[0m[0;34m([0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m   1302 [0m                [0mloss[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mcompute_loss[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0minputs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m   1303 [0m        [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0;32m-> 1304 [0;31m            [0mloss[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mcompute_loss[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0minputs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m   1305 [0m[0;34m[0m[0m
[1;32m   1306 [0m        [0;32mif[0m [0mself[0m[0;34m.[0m[0margs[0m[0;34m.[0m[0mn_gpu[0m [0;34m>[0m [0;36m1[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m   1307 [0m            [0mloss[0m [0;34m=[0m [0mloss[0m[0;34m.[0m[0mmean[0m[0;34m([0m[0;34m)[0m  [0;31m# mean() to average on multi-gpu parallel training[0m[0;34m[0m[0;34m[0m[0m
[1;32m   1308 [0m[0;34m[0m[0m
[1;32m   1309 [0m        [0;32mif[0m [0mself[0m[0;34m.[0m[0margs[0m[0;34m.[0m[0mgradient_accumulation_steps[0m [0;34m>[0m [0;36m1[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m

> [0;32m/home/zijiao/anaconda3/envs/dnlp/lib/python3.8/site-packages/transformers/trainer.py[0m(940)[0;36mtrain[0;34m()[0m
[0;32m    939 [0;31m                [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 940 [0;31m                    [0mtr_loss[0m [0;34m+=[0m [0mself[0m[0;34m.[0m[0mtraining_step[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0minputs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    941 [0;31m                [0mself[0m[0;34m.[0m[0m_total_flos[0m [0;34m+=[0m [0mself[0m[0;34m.[0m[0mfloating_point_ops[0m[0;34m([0m[0minputs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
[1;32m    935 [0m                [0;32mif[0m [0;34m([0m[0;34m([0m[0mstep[0m [0;34m+[0m [0;36m1[0m[0;34m)[0m [0;34m%[0m [0mself[0m[0;34m.[0m[0margs[0m[0;34m.[0m[0mgradient_accumulation_steps[0m [0;34m!=[0m [0;36m0[0m[0;34m)[0m [0;32mand[0m [0mself[0m[0;34m.[0m[0margs[0m[0;34m.[0m[0mlocal_rank[0m [0;34m!=[0m [0;34m-[0m[0;36m1[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m    936 [0m                    [0;31m# Avoid unnecessary DDP synchronization since there will be no backward pass on this example.[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[1;32m    937 [0m                    [0;32mwith[0m [0mmodel[0m[0;34m.[0m[0mno_sync[0m[0;34m([0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m    938 [0m                        [0mtr_loss[0m [0;34m+=[0m [0mself[0m[0;34m.[0m[0mtraining_step[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0minputs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m    939 [0m                [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 940 [0;31m                    [0mtr_loss[0m [0;34m+=[0m [0mself[0m[0;34m.[0m[0mtraining_step[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0minputs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    941 [0m                [0mself[0m[0;34m.[0m[0m_total_flos[0m [0;34m+=[0m [0mself[0m[0;34m.[0m[0mfloating_point_ops[0m[0;34m([0m[0minputs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m    942 [0m[0;34m[0m[0m
[1;32m    943 [0m                if (step + 1) % self.args.gradient_accumulation_steps == 0 or (
[1;32m    944 [0m                    [0;31m# last step in epoch but step is always smaller than gradient_accumulation_steps[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[1;32m    945 [0m                    [0msteps_in_epoch[0m [0;34m<=[0m [0mself[0m[0;34m.[0m[0margs[0m[0;34m.[0m[0mgradient_accumulation_steps[0m[0;34m[0m[0;34m[0m[0m

> [0;32m/home/zijiao/anaconda3/envs/dnlp/lib/python3.8/site-packages/transformers/trainer.py[0m(1304)[0;36mtraining_step[0;34m()[0m
[0;32m   1303 [0;31m        [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m-> 1304 [0;31m            [0mloss[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mcompute_loss[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0minputs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m   1305 [0;31m[0;34m[0m[0m
[0m
{'input_ids': tensor([[50259,    40,  1101,   407, 20536,   284,   423,   616,  1641,  3187,
            13, 50260, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,
         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,
         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,
         50256, 50256, 50256, 50256, 50256, 50256, 50257, 35474,   329,  1641,
         10013,  1303,    86,  2238,  1303,   400,    81,  2967,  2638,  1378,
            83,    13,  1073,    14,    52,  2953, 41473, 16072,    65,    50,
            23, 50258, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,
         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,
         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]],
       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1]], device='cuda:0'), 'labels': tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100, 50257, 35474,   329,  1641,
         10013,  1303,    86,  2238,  1303,   400,    81,  2967,  2638,  1378,
            83,    13,  1073,    14,    52,  2953, 41473, 16072,    65,    50,
            23, 50258,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]],
       device='cuda:0')}

